# House Rent Prediction and Analysis

## Project Overview

This project aims to predict house rent prices based on various features of the property. It includes data preprocessing, feature engineering, model training, feature importance analysis, and deployment of the prediction model as a web API using Flask, both locally and on AWS Lambda.

## Features

*   **Data Preprocessing**: Cleans and prepares the house rent dataset for modeling, including outlier removal and floor information extraction.
*   **Model Training**: Trains a Random Forest Regressor model to predict rent prices.
*   **Feature Importance**: Analyzes feature importances to understand key drivers of rent.
*   **Local API**: A Flask web application to serve predictions locally.
*   **AWS Lambda Deployment**: The Flask application is packaged using Docker and deployed to AWS Lambda for serverless predictions, with model artifacts loaded from S3.

## Directory Structure

```
CPE393-group_name/
├── app/
│   ├── data_processing.py      # Data preprocessing for model training & local API
│   ├── model_training.py       # Script to train and save the prediction model
│   ├── app.py                  # Flask application for local predictions
│   └── templates/
│       └── index.html          # Simple HTML frontend for local API
├── data/
│   ├── House_Rent_Dataset.csv  # Main dataset
│   └── Dataset Glossary.txt    # Glossary for dataset columns
├── deploy/
│   ├── data_processing.py      # Data preprocessing for Lambda (uses pre-fitted encoders)
│   ├── lambda_function.py      # Flask app & Lambda handler, loads model from S3
│   ├── dockerfile              # Dockerfile for AWS Lambda deployment
│   ├── requirements.txt        # Python dependencies for Lambda
│   └── templates/
│       └── index.html          # Simple HTML frontend for Lambda API
├── models/                     # (Generated by app/model_training.py)
│   ├── RandomForestRegressor.pkl # Trained model
│   ├── standard_scaler.pkl     # Fitted scaler
│   └── onehot_encoder.pkl      # Fitted one-hot encoder
├── feature_ranking.ipynb       # Jupyter notebook for feature importance analysis
├── feature_ranking.py          # Python script for feature importance analysis
├── .gitignore                  # Specifies intentionally untracked files for Git
└── README.md                   # This file
```

## Detailed File Descriptions

### Root Directory

*   **`.gitignore`**: Specifies intentionally untracked files that Git should ignore (e.g., `__pycache__`, virtual environments).
*   **`feature_ranking.ipynb`**: A Jupyter Notebook for interactive exploration, data preprocessing (including floor feature engineering), and determining feature importance using a Random Forest model.
*   **`feature_ranking.py`**: Python script version of the feature ranking notebook. It loads the dataset, performs preprocessing, and prints the feature importance ranking.
*   **`README.md`**: This documentation file.

### `data/`

*   **`House_Rent_Dataset.csv`**: The primary dataset containing features of rental properties and their rent.
*   **`Dataset Glossary.txt`**: Provides definitions for each column in the dataset.

### `app/` (Local Development & Model Training)

*   **`data_processing.py`**:
    *   Contains the `DataProcessor` class for preprocessing data before model training and for the local API.
    *   Handles outlier removal, extraction of `CurrentFloor` and `TotalFloors` from the 'Floor' column, numerical scaling (StandardScaler), and categorical encoding (OneHotEncoder).
    *   Fits and saves the scaler and encoder to the `models/` directory during the training data processing.
*   **`model_training.py`**:
    *   Uses `app/data_processing.py` to process the training data.
    *   Trains a `RandomForestRegressor` model (and optionally a `LinearRegression` model).
    *   Evaluates the model and saves the trained model (`RandomForestRegressor.pkl`), scaler (`standard_scaler.pkl`), and encoder (`onehot_encoder.pkl`) to the `../models/` directory.
*   **`app.py`**:
    *   A Flask web application for serving rent predictions locally.
    *   Loads the trained `RandomForestRegressor.pkl` and the preprocessors from the `models/` directory.
    *   Provides API endpoints for prediction, health checks, and model information.
*   **`templates/index.html`**: A basic HTML page for interacting with the local Flask API.

### `models/` (Generated by `app/model_training.py`)

*   **`RandomForestRegressor.pkl`**: The serialized, trained Random Forest Regressor model.
*   **`standard_scaler.pkl`**: The serialized, fitted StandardScaler object for numerical features.
*   **`onehot_encoder.pkl`**: The serialized, fitted OneHotEncoder object for categorical features.
    *These files are used by `app/app.py` for local predictions and need to be uploaded to S3 for the Lambda deployment.*

### `deploy/` (AWS Lambda Deployment)

*   **`data_processing.py`**:
    *   Contains a `DataProcessor` class tailored for the Lambda environment.
    *   Loads pre-fitted scaler and encoder (downloaded from S3 by `lambda_function.py`).
    *   Prepares input data for prediction using these loaded preprocessors. It expects 'CurrentFloor' and 'TotalFloors' to be provided in the input.
*   **`lambda_function.py`**:
    *   The entry point for the AWS Lambda function.
    *   Uses `awsgi` to wrap a Flask application, making it compatible with Lambda and API Gateway.
    *   Downloads the model, scaler, and encoder from an S3 bucket (`osp-ai-inventrack-nonprod`) to the Lambda's `/tmp` directory during cold starts.
    *   Provides API endpoints similar to the local `app.py`.
*   **`dockerfile`**:
    *   A multi-stage Dockerfile used to build the deployment package for AWS Lambda.
    *   Installs dependencies from `requirements.txt` and packages the application code (`lambda_function.py`, `deploy/data_processing.py`, `templates/`).
*   **`requirements.txt`**: Lists Python dependencies required for the Lambda function (e.g., `flask`, `awsgi`, `boto3`, `joblib`, `scikit-learn`, `numpy`, `pandas`).
*   **`templates/index.html`**: A basic HTML page, potentially served by the Lambda function if the `/` route is accessed.

## Setup and Usage

### Prerequisites

*   Python 3.9+
*   pip (Python package installer)
*   Docker (for Lambda deployment)
*   AWS CLI (configured, for Lambda deployment)

### Data Preparation

1.  Ensure the `House_Rent_Dataset.csv` and `Dataset Glossary.txt` files are present in the `data/` directory.

### Model Training (Local)

1.  Navigate to the `app/` directory:
    ```bash
    cd app
    ```
2.  Install dependencies (you might want to use a virtual environment):
    ```bash
    pip install scikit-learn pandas numpy joblib
    ```
3.  Run the model training script:
    ```bash
    python model_training.py
    ```
    This will process the data, train the `RandomForestRegressor`, and save `RandomForestRegressor.pkl`, `standard_scaler.pkl`, and `onehot_encoder.pkl` into a `models/` directory at the project root (i.e., `CPE393-group_name/models/`).

### Running Local API (Flask)

1.  Ensure models have been trained and are in the `models/` directory.
2.  Navigate to the `app/` directory if not already there.
3.  Install Flask if you haven't:
    ```bash
    pip install Flask
    ```
4.  Run the Flask application:
    ```bash
    python app.py
    ```
5.  The API will be available at `http://localhost:5000`.

### Deployment to AWS Lambda

1.  **Upload Artifacts to S3**:
    *   Upload the generated `RandomForestRegressor.pkl`, `standard_scaler.pkl`, and `onehot_encoder.pkl` from your local `models/` directory to your S3 bucket: `s3://osp-ai-inventrack-nonprod/model/`.
    *   Ensure the S3 paths in `deploy/lambda_function.py` match your S3 object keys.

2.  **Build and Push Docker Image**:
    *   Navigate to the `deploy/` directory:
        ```bash
        cd ../deploy
        ```
    *   Build the Docker image (replace `your-aws-account-id`, `your-region`, and `your-ecr-repo-name`):
        ```bash
        docker build -t your-ecr-repo-name .
        aws ecr get-login-password --region your-region | docker login --username AWS --password-stdin your-aws-account-id.dkr.ecr.your-region.amazonaws.com
        docker tag your-ecr-repo-name:latest your-aws-account-id.dkr.ecr.your-region.amazonaws.com/your-ecr-repo-name:latest
        docker push your-aws-account-id.dkr.ecr.your-region.amazonaws.com/your-ecr-repo-name:latest
        ```

3.  **Create/Update Lambda Function**:
    *   In the AWS Management Console, create a new Lambda function or update an existing one.
    *   Choose "Container image" as the type.
    *   Provide the ECR image URI from the previous step.
    *   Configure environment variables, memory, timeout, and IAM role permissions (the role needs S3 read access to the bucket containing model artifacts and permissions for CloudWatch Logs).
    *   Set up an API Gateway trigger for the Lambda function to expose it as an HTTP API.

### Feature Ranking Analysis

1.  Navigate to the project root directory.
2.  To run the script:
    ```bash
    python feature_ranking.py
    ```
3.  Alternatively, open and run the cells in `feature_ranking.ipynb` using Jupyter Notebook or JupyterLab.

## API Endpoints

The input JSON for prediction endpoints should have the following structure:
```json
{
    "BHK": 2,
    "Size": 1000,
    "Area Type": "Super Area",
    "City": "Kolkata",
    "Furnishing Status": "Semi-Furnished",
    "Tenant Preferred": "Bachelors/Family",
    "Bathroom": 2,
    "Point of Contact": "Contact Owner",
    "CurrentFloor": 1,
    "TotalFloors": 3
}
```

### Local Flask API (`app/app.py`)

*   **`GET /`**:
    *   Description: Serves a simple HTML page for interacting with the API.
*   **`POST /predict`**:
    *   Description: Predicts house rent.
    *   Request Body: JSON object with property features (see example above).
    *   Response: JSON object with the prediction and a confidence interval.
      ```json
      {
          "success": true,
          "prediction": {
              "prediction": 15000.00,
              "confidence_interval": {
                  "lower": 12000.00,
                  "upper": 18000.00
              }
          }
      }
      ```
*   **`GET /health`**:
    *   Description: Health check endpoint.
    *   Response: `{"status": "healthy"}`
*   **`GET /model_info`**:
    *   Description: Provides information about the model.
    *   Response: JSON with model type, version, and features.

### AWS Lambda API (`deploy/lambda_function.py` via API Gateway)

The paths will depend on your API Gateway stage and configuration. Assuming the base path maps to the Flask app:

*   **`GET /`**: (If API Gateway route is configured)
    *   Description: Serves the HTML page.
*   **`POST /predict`**: (If API Gateway route is configured)
    *   Description: Predicts house rent.
    *   Request Body: JSON object with property features (same as local API).
    *   Response: JSON object with the prediction and confidence interval (same as local API).

```// filepath: c:\Users\worapolk\Documents\GitHub\CPE393-group_name\README.md
# House Rent Prediction and Analysis

## Project Overview

This project aims to predict house rent prices based on various features of the property. It includes data preprocessing, feature engineering, model training, feature importance analysis, and deployment of the prediction model as a web API using Flask, both locally and on AWS Lambda.

## Features

*   **Data Preprocessing**: Cleans and prepares the house rent dataset for modeling, including outlier removal and floor information extraction.
*   **Model Training**: Trains a Random Forest Regressor model to predict rent prices.
*   **Feature Importance**: Analyzes feature importances to understand key drivers of rent.
*   **Local API**: A Flask web application to serve predictions locally.
*   **AWS Lambda Deployment**: The Flask application is packaged using Docker and deployed to AWS Lambda for serverless predictions, with model artifacts loaded from S3.

## Directory Structure

```
CPE393-group_name/
├── app/
│   ├── data_processing.py      # Data preprocessing for model training & local API
│   ├── model_training.py       # Script to train and save the prediction model
│   ├── app.py                  # Flask application for local predictions
│   └── templates/
│       └── index.html          # Simple HTML frontend for local API
├── data/
│   ├── House_Rent_Dataset.csv  # Main dataset
│   └── Dataset Glossary.txt    # Glossary for dataset columns
├── deploy/
│   ├── data_processing.py      # Data preprocessing for Lambda (uses pre-fitted encoders)
│   ├── lambda_function.py      # Flask app & Lambda handler, loads model from S3
│   ├── dockerfile              # Dockerfile for AWS Lambda deployment
│   ├── requirements.txt        # Python dependencies for Lambda
│   └── templates/
│       └── index.html          # Simple HTML frontend for Lambda API
├── models/                     # (Generated by app/model_training.py)
│   ├── RandomForestRegressor.pkl # Trained model
│   ├── standard_scaler.pkl     # Fitted scaler
│   └── onehot_encoder.pkl      # Fitted one-hot encoder
├── feature_ranking.ipynb       # Jupyter notebook for feature importance analysis
├── feature_ranking.py          # Python script for feature importance analysis
├── .gitignore                  # Specifies intentionally untracked files for Git
└── README.md                   # This file
```

## Detailed File Descriptions

### Root Directory

*   **`.gitignore`**: Specifies intentionally untracked files that Git should ignore (e.g., `__pycache__`, virtual environments).
*   **`feature_ranking.ipynb`**: A Jupyter Notebook for interactive exploration, data preprocessing (including floor feature engineering), and determining feature importance using a Random Forest model.
*   **`feature_ranking.py`**: Python script version of the feature ranking notebook. It loads the dataset, performs preprocessing, and prints the feature importance ranking.
*   **`README.md`**: This documentation file.

### `data/`

*   **`House_Rent_Dataset.csv`**: The primary dataset containing features of rental properties and their rent.
*   **`Dataset Glossary.txt`**: Provides definitions for each column in the dataset.

### `app/` (Local Development & Model Training)

*   **`data_processing.py`**:
    *   Contains the `DataProcessor` class for preprocessing data before model training and for the local API.
    *   Handles outlier removal, extraction of `CurrentFloor` and `TotalFloors` from the 'Floor' column, numerical scaling (StandardScaler), and categorical encoding (OneHotEncoder).
    *   Fits and saves the scaler and encoder to the `models/` directory during the training data processing.
*   **`model_training.py`**:
    *   Uses `app/data_processing.py` to process the training data.
    *   Trains a `RandomForestRegressor` model (and optionally a `LinearRegression` model).
    *   Evaluates the model and saves the trained model (`RandomForestRegressor.pkl`), scaler (`standard_scaler.pkl`), and encoder (`onehot_encoder.pkl`) to the `../models/` directory.
*   **`app.py`**:
    *   A Flask web application for serving rent predictions locally.
    *   Loads the trained `RandomForestRegressor.pkl` and the preprocessors from the `models/` directory.
    *   Provides API endpoints for prediction, health checks, and model information.
*   **`templates/index.html`**: A basic HTML page for interacting with the local Flask API.

### `models/` (Generated by `app/model_training.py`)

*   **`RandomForestRegressor.pkl`**: The serialized, trained Random Forest Regressor model.
*   **`standard_scaler.pkl`**: The serialized, fitted StandardScaler object for numerical features.
*   **`onehot_encoder.pkl`**: The serialized, fitted OneHotEncoder object for categorical features.
    *These files are used by `app/app.py` for local predictions and need to be uploaded to S3 for the Lambda deployment.*

### `deploy/` (AWS Lambda Deployment)

*   **`data_processing.py`**:
    *   Contains a `DataProcessor` class tailored for the Lambda environment.
    *   Loads pre-fitted scaler and encoder (downloaded from S3 by `lambda_function.py`).
    *   Prepares input data for prediction using these loaded preprocessors. It expects 'CurrentFloor' and 'TotalFloors' to be provided in the input.
*   **`lambda_function.py`**:
    *   The entry point for the AWS Lambda function.
    *   Uses `awsgi` to wrap a Flask application, making it compatible with Lambda and API Gateway.
    *   Downloads the model, scaler, and encoder from an S3 bucket (`osp-ai-inventrack-nonprod`) to the Lambda's `/tmp` directory during cold starts.
    *   Provides API endpoints similar to the local `app.py`.
*   **`dockerfile`**:
    *   A multi-stage Dockerfile used to build the deployment package for AWS Lambda.
    *   Installs dependencies from `requirements.txt` and packages the application code (`lambda_function.py`, `deploy/data_processing.py`, `templates/`).
*   **`requirements.txt`**: Lists Python dependencies required for the Lambda function (e.g., `flask`, `awsgi`, `boto3`, `joblib`, `scikit-learn`, `numpy`, `pandas`).
*   **`templates/index.html`**: A basic HTML page, potentially served by the Lambda function if the `/` route is accessed.

## Setup and Usage

### Prerequisites

*   Python 3.9+
*   pip (Python package installer)
*   Docker (for Lambda deployment)
*   AWS CLI (configured, for Lambda deployment)

### Data Preparation

1.  Ensure the `House_Rent_Dataset.csv` and `Dataset Glossary.txt` files are present in the `data/` directory.

### Model Training (Local)

1.  Navigate to the `app/` directory:
    ```bash
    cd app
    ```
2.  Install dependencies (you might want to use a virtual environment):
    ```bash
    pip install scikit-learn pandas numpy joblib
    ```
3.  Run the model training script:
    ```bash
    python model_training.py
    ```
    This will process the data, train the `RandomForestRegressor`, and save `RandomForestRegressor.pkl`, `standard_scaler.pkl`, and `onehot_encoder.pkl` into a `models/` directory at the project root (i.e., `CPE393-group_name/models/`).

### Running Local API (Flask)

1.  Ensure models have been trained and are in the `models/` directory.
2.  Navigate to the `app/` directory if not already there.
3.  Install Flask if you haven't:
    ```bash
    pip install Flask
    ```
4.  Run the Flask application:
    ```bash
    python app.py
    ```
5.  The API will be available at `http://localhost:5000`.

### Deployment to AWS Lambda

1.  **Upload Artifacts to S3**:
    *   Upload the generated `RandomForestRegressor.pkl`, `standard_scaler.pkl`, and `onehot_encoder.pkl` from your local `models/` directory to your S3 bucket: `s3://osp-ai-inventrack-nonprod/model/`.
    *   Ensure the S3 paths in `deploy/lambda_function.py` match your S3 object keys.

2.  **Build and Push Docker Image**:
    *   Navigate to the `deploy/` directory:
        ```bash
        cd ../deploy
        ```
    *   Build the Docker image (replace `your-aws-account-id`, `your-region`, and `your-ecr-repo-name`):
        ```bash
        docker build -t your-ecr-repo-name .
        aws ecr get-login-password --region your-region | docker login --username AWS --password-stdin your-aws-account-id.dkr.ecr.your-region.amazonaws.com
        docker tag your-ecr-repo-name:latest your-aws-account-id.dkr.ecr.your-region.amazonaws.com/your-ecr-repo-name:latest
        docker push your-aws-account-id.dkr.ecr.your-region.amazonaws.com/your-ecr-repo-name:latest
        ```

3.  **Create/Update Lambda Function**:
    *   In the AWS Management Console, create a new Lambda function or update an existing one.
    *   Choose "Container image" as the type.
    *   Provide the ECR image URI from the previous step.
    *   Configure environment variables, memory, timeout, and IAM role permissions (the role needs S3 read access to the bucket containing model artifacts and permissions for CloudWatch Logs).
    *   Set up an API Gateway trigger for the Lambda function to expose it as an HTTP API.

### Feature Ranking Analysis

1.  Navigate to the project root directory.
2.  To run the script:
    ```bash
    python feature_ranking.py
    ```
3.  Alternatively, open and run the cells in `feature_ranking.ipynb` using Jupyter Notebook or JupyterLab.

## API Endpoints

The input JSON for prediction endpoints should have the following structure:
```json
{
    "BHK": 2,
    "Size": 1000,
    "Area Type": "Super Area",
    "City": "Kolkata",
    "Furnishing Status": "Semi-Furnished",
    "Tenant Preferred": "Bachelors/Family",
    "Bathroom": 2,
    "Point of Contact": "Contact Owner",
    "CurrentFloor": 1,
    "TotalFloors": 3
}
```

### Local Flask API (`app/app.py`)

*   **`GET /`**:
    *   Description: Serves a simple HTML page for interacting with the API.
*   **`POST /predict`**:
    *   Description: Predicts house rent.
    *   Request Body: JSON object with property features (see example above).
    *   Response: JSON object with the prediction and a confidence interval.
      ```json
      {
          "success": true,
          "prediction": {
              "prediction": 15000.00,
              "confidence_interval": {
                  "lower": 12000.00,
                  "upper": 18000.00
              }
          }
      }
      ```
*   **`GET /health`**:
    *   Description: Health check endpoint.
    *   Response: `{"status": "healthy"}`
*   **`GET /model_info`**:
    *   Description: Provides information about the model.
    *   Response: JSON with model type, version, and features.

### AWS Lambda API (`deploy/lambda_function.py` via API Gateway)

The paths will depend on your API Gateway stage and configuration. Assuming the base path maps to the Flask app:

*   **`GET /`**: (If API Gateway route is configured)
    *   Description: Serves the HTML page.
*   **`POST /predict`**: (If API Gateway route is configured)
    *   Description: Predicts house rent.
    *   Request Body: JSON object with property features (same as local API).
    *   Response: JSON object with the prediction and confidence interval (same as local API).
